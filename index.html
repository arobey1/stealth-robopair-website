<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PT2SMHYS4B"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-PT2SMHYS4B');
    </script>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/x-icon" href="../img/icons/soccerball.png">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Jailbreaking LLM-controlled robots">
    <meta name="twitter:description" content="Robots can be jailbroken. Read on if you want to learn how.">
    <meta name="twitter:image" content="https://robopair.org/img/writing/jailbreaking-robots/robodog.png">

    <title>RoboPAIR</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        
        <main>
            <div class="research-page-heading-container">
                <h1 class="research-page-title">Jailbreaking LLM-Controlled Robots</h1>
                <div class="research-page-meta">
                    <p class="research-page-author">
                        Alexander Robey, Zachary Ravichandran,<br> Vijay Kumar, Hamed Hassani, George J. Pappas
                    </p>
                </div>
            </div>
            

            <div class="social-icons">
                <a href="https://arxiv.org/pdf/2410.13691" target="_blank">
                    <img src="../../img/icons/pdf-black.png" alt="Paper">
                    <div>[arXiv paper]</div>
                </a>
                <a href="https://x.com/AlexRobey23/status/1846914890029748272">
                    <img src="../../img/icons/twitter-black.png" alt="Twitter">
                    <div>[Twitter thread]</div>
                </a>
                <a href="https://arobey1.github.io/writing/jailbreakingrobots.html">
                    <img src="../../img/icons/blog-black.png" alt="Cap">
                    <div>[Blog post]</div>
                </a>
                <!-- <a href="https://github.com/">
                    <img src="../../img/icons/github-black.png" alt="GitHub">
                    <div>[Source code]</div>
                </a> -->
            </div>

            <div class="research-video-with-caption">
                <video controls>
                  <source src="files/research/bomb.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <div class="caption">
                    <span class="figure-label">Figure 1:</span> 
                    <span class="caption-text">Jailbreaking the Unitree Go2 robot dog.</span>
                </div>
            </div>
            
            <div class="research-page-content">

                <p>
                    <span class="bold-text">Summary.</span> The recent introduction of large language models (LLMs) has revolutionized the field of robotics by enabling contextual reasoning and intuitive human-robot interaction in domains as varied as manipulation, locomotion, and self-driving vehicles.  When viewed as a stand-alone technology, LLMs are known to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit harmful text by bypassing LLM safety guardrails.  To assess the risks of deploying LLMs in robotics, in this paper, we introduce RoboPAIR, the first algorithm designed to jailbreak LLM-controlled robots.  Unlike existing, textual attacks on LLM chatbots, RoboPAIR elicits harmful physical actions from LLM-controlled robots, a phenomenon we experimentally demonstrate in three scenarios: (i) a white-box setting, wherein the attacker has full access to the NVIDIA Dolphins self-driving LLM, (ii) a gray-box setting, wherein the attacker has partial access to a Clearpath Robotics Jackal UGV robot equipped with a GPT-4o planner, and (iii) a black-box setting, wherein the attacker has only query access to the GPT-3.5-integrated Unitree Robotics Go2 robot dog. In each scenario and across three new datasets of harmful robotic actions, we demonstrate that RoboPAIR, as well as several static baselines, finds jailbreaks quickly and effectively, often achieving 100% attack success rates.  Our results reveal, for the first time, that the risks of jailbroken LLMs extend far beyond text generation, given the distinct possibility that jailbroken robots could cause physical damage in the real world. Indeed, our results on the Unitree Go2 represent the first successful jailbreak of a deployed commercial robotic system. Addressing this emerging vulnerability is critical for ensuring the safe deployment of LLMs in robotics.
                </p>

                <p>
                    <span class="bold-text">Responsible disclosure.</span> Prior to the public release of this work, we shared our findings with leading AI companies as well as the manufacturers of the robots used in this study.
                </p>

                <figure class="image-with-caption">
                    <img src="img/writing/jailbreaking-robots/robopair-poster.jpg" alt="Refusal GIF" class="wide-image">
                    <figcaption>
                        <div class="caption">
                            <span class="figure-label">Figure 2:</span> 
                            <span class="caption-text">An overview of this research.</span>
                        </div>
                    </figcaption>
                </figure> 
                
                <p>
                    <span class="bold-text">BibTex.</span> 
                    If you find our work useful in your research, please consider citing our paper.
                </p>
                <div class="bibtex-container">
                    
                <pre>
@article{robey2024jailbreaking,
    title={Jailbreaking LLM-Controlled Robots},
    author={Robey, Alexander and Ravichandran, Zachary and Kumar, Vijay and Hassani, Hamed and Pappas, George J.},
    journal={arXiv preprint arXiv:2410.13691},
    year={2024}
}</pre>
                </div>
            </div>
                
        </main>
    </div>
</body>
</html>
